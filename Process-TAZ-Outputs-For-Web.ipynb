{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- new data from remm se output\n",
    "- old data from wfrc data portal\n",
    "- before running I standarized the column names\n",
    "- Filter city areas\n",
    "\n",
    "Create a Feature Layer for each combination of Map Display Geography (4) x Variables (7) wherein the attribute fields are \n",
    "\n",
    "  Like this for Households (for the other 6 variables substitute hh_ with: pop_, ijb_, ojb, rjb_, tjb_,  hji_)\n",
    "\n",
    "TAZID, city, county, med_dist, small_dist, dev_acres, hh_19, hh_20, hh_21…hh_50…old_hh_20, old_hh_25, old_hh_50\n",
    "Sml_DistID, county, dev_acres, hh_19, hh_20, hh_21…hh_50…old_hh_20, old_hh_25, old_hh_50\n",
    "Med_DistID, county, dev_acres, hh_19, hh_20, hh_21…hh_50…old_hh_20, old_hh_25, old_hh_50\n",
    "CityID, county, dev_acres, hh_19, hh_20, hh_21…hh_50…old_hh_20, old_hh_25, old_hh_50\n",
    "\n",
    "# How the metrics are calculated:\n",
    "\n",
    "tdm_output['ALLEMP'] = tdm_output['RETL']+tdm_output['FOOD']+tdm_output['MANU']+tdm_output['WSLE']+tdm_output['OFFI']+tdm_output['GVED']+tdm_output['HLTH']+tdm_output['OTHR']+tdm_output['FM_AGRI']+tdm_output['FM_MING']+tdm_output['FM_CONS']+tdm_output['HBJ']\n",
    "    \n",
    "    tdm_output['RETEMP'] = tdm_output['RETL'] + tdm_output['FOOD']\n",
    "    tdm_output['INDEMP'] = tdm_output['MANU'] + tdm_output['WSLE']\n",
    "    tdm_output['OTHEMP'] = tdm_output['OFFI'] + tdm_output['GVED']+ tdm_output['HLTH'] + tdm_output['OTHR']\n",
    "    tdm_output['TOTEMP'] = tdm_output['RETEMP'] + tdm_output['INDEMP']+ tdm_output['OTHEMP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from arcpy import env\n",
    "import os\n",
    "import glob\n",
    "from arcgis import GIS\n",
    "from arcgis.features import GeoAccessor\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from datetime import date\n",
    "import shutil\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.parallelProcessingFactor = \"90%\"\n",
    "\n",
    "# show all columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# pd.DataFrame.spatial.from_featureclass(???)\n",
    "# df.spatial.to_featureclass(location=???,sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# parcels = r'.\\\\Inputs\\\\remm_base_year_20220513.gdb\\\\parcels_2019'\n",
    "taz832 = r\".\\Inputs\\TAZ_832_ID_Only.shp\"\n",
    "# taz900 = r\".\\Inputs\\TAZ_900.shp\"\n",
    "taz900 = r\".\\Inputs\\TAZ_900.gdb\\TAZ\"\n",
    "city_areas = r\".\\Inputs\\TAZ_900.gdb\\CITYAREA\"\n",
    "med_districts = r\".\\Inputs\\TAZ_900.gdb\\DISTMED\"\n",
    "counties = r\".\\Inputs\\TAZ_900.gdb\\COUNTY\"\n",
    "# small_districts = ???\n",
    "# regions_df = pd.DataFrame.spatial.from_featureclass(regions_shp)\n",
    "\n",
    "taz832_sdf = pd.DataFrame.spatial.from_featureclass(taz832)\n",
    "taz900_sdf = pd.DataFrame.spatial.from_featureclass(taz900)\n",
    "city_areas_sdf = pd.DataFrame.spatial.from_featureclass(city_areas)[['CityArea','SHAPE']].copy()\n",
    "med_districts_sdf = pd.DataFrame.spatial.from_featureclass(med_districts)[['DISTMED','SHAPE']].copy()\n",
    "counties_sdf = pd.DataFrame.spatial.from_featureclass(counties)[['CO_NAME','SHAPE']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the paths to the new SE outputs\n",
    "pm_folder = r\".\\New_SE_Data\"\n",
    "new_taz_se = glob.glob(os.path.join(pm_folder,'SE_WF_*.csv'))\n",
    "len(new_taz_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the paths to the old SE outputs\n",
    "old_se_folder = r\".\\Old_SE_Data\"\n",
    "old_taz_se = glob.glob(os.path.join(old_se_folder,'*_TAZ.csv'))\n",
    "old_city_se = glob.glob(os.path.join(old_se_folder,'*_City_Area.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = r'.\\Outputs'\n",
    "if not os.path.exists(outputs):\n",
    "    os.makedirs(outputs)\n",
    "\n",
    "scratch = os.path.join(outputs, 'scratch.gdb')\n",
    "if not arcpy.Exists(scratch):\n",
    "    arcpy.CreateFileGDB_management(outputs, 'scratch.gdb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = os.path.splitext(os.path.basename(new_taz_se[0]))[0].split('_')[-1]\n",
    "# x = pd.read_csv(new_taz_se[0])\n",
    "# x['OFFI'] = x['OFFI'] + x['GVED'] + x['HLTH']\n",
    "# x['TPCL'] = x['OFFI'] + x['RETEMP'] + x['INDEMP']\n",
    "# x['HJI'] = x['TOTHH']*1.8 + x['TOTEMP']\n",
    "# x = x[[';TAZID', 'TOTHH', 'HHPOP', 'TOTEMP','RETEMP', 'INDEMP', 'OFFI', 'TPCL', 'HJI']].copy()\n",
    "# x.columns = ['TAZID', f'HH_{year}', f'POP_{year}', f'EMP_{year}', f'RTL_{year}', f'IND_{year}', f'OFFI_{year}', f'TPCL_{year}', f'HJI_{year}']\n",
    "# x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process New SE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = taz900_sdf[['SA_TAZID', 'DISTMED', 'CityArea', 'CO_NAME','SHAPE']].copy()\n",
    "base.shape\n",
    "\n",
    "for csv in new_taz_se:\n",
    "\n",
    "    year = os.path.splitext(os.path.basename(csv))[0].split('_')[-1]\n",
    "    df = pd.read_csv(csv)\n",
    "    df['TPCL'] = df['TOTEMP']\n",
    "    df['HJI'] = df['TOTHH']*1.8 + df['TPCL']\n",
    "    df = df[[';TAZID', 'TOTHH', 'HHPOP', 'ALLEMP','RETEMP', 'INDEMP', 'OTHEMP', 'TPCL', 'HJI']].copy()\n",
    "    df.columns = ['SA_TAZID', f'HH_{year}', f'POP_{year}', f'AEMP_{year}', f'RTL_{year}', f'IND_{year}', f'OTHR_{year}', f'TPCL_{year}', f'HJI_{year}']\n",
    "    \n",
    "    base = base.merge(df, on='SA_TAZID', how='left')\n",
    "\n",
    "new_se = base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3546, 261)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_se.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\REMM-Process-SE-Output-For-Web\\\\Outputs\\\\scratch.gdb\\\\_01_new_se_taz_900'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "new_se_taz_900 = os.path.join(scratch, '_01_new_se_taz_900')\n",
    "new_se.spatial.to_featureclass(location=new_se_taz_900,sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Old SE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_dict =  {'TOTHH':'HH',    \n",
    "            'HHPOP':'POP',\n",
    "            'RETEMP':'RTL',\n",
    "            'TOTEMP':'TPCL',\n",
    "            'ALLEMP':'AEMP',\n",
    "            'INDEMP':'IND',\n",
    "            'OTHEMP':'OTHR'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv(old_taz_se[0])\n",
    "# category = se_dict[test.iloc[0]['SECategory']]\n",
    "# val_cols = [col for col in list(test.columns) if 'YEAR' in col and  'D' not in col and int(col[4:]) >= 2019]\n",
    "# test = test[['CO_TAZID'] + val_cols]\n",
    "# new_val_cols = [col.replace('YEAR',category + '_') for col in val_cols]\n",
    "# test.columns = ['CO_TAZID'] + new_val_cols\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_base = taz832_sdf[['CO_TAZID','SHAPE']].copy()\n",
    "old_base.shape\n",
    "\n",
    "for csv in old_taz_se:\n",
    "\n",
    "    df = pd.read_csv(csv)\n",
    "    category = se_dict[df.iloc[0]['SECategory']]\n",
    "    val_cols = [col for col in list(df.columns) if 'YEAR' in col and  'D' not in col and int(col[4:]) >= 2019]\n",
    "    df = df[['CO_TAZID'] + val_cols]\n",
    "    new_val_cols = [col.replace('YEAR',category + '_') for col in val_cols]\n",
    "    df.columns = ['CO_TAZID'] + new_val_cols\n",
    "    old_base = old_base.merge(df, on='CO_TAZID',how='left')\n",
    "\n",
    "old_se = old_base\n",
    "old_se.rename({'CO_TAZID': 'COTAZID832'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate household jobs index\n",
    "for year in range(2019,2051):\n",
    "    old_se['HJI_{}'.format(year)] = (old_se['HH_{}'.format(year)] * 1.8) + old_se['TPCL_{}'.format(year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 258)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_se.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\REMM-Process-SE-Output-For-Web\\\\Outputs\\\\scratch.gdb\\\\_02_old_se_taz_832'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "old_se_taz_832 = os.path.join(scratch, '_02_old_se_taz_832')\n",
    "old_se.spatial.to_featureclass(location=old_se_taz_832,sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare output dimensions\n",
    "old_se.shape == new_se.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Monday, July 11, 2022 1:49:04 PM\",\"Succeeded at Monday, July 11, 2022 1:50:28 PM (Elapsed Time: 1 minutes 23 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result '.\\\\Outputs\\\\scratch.gdb\\\\_03_taz832_taz900_identity'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge geometry between taz 832 and output geometries (taz900, )\n",
    "taz900_identity = arcpy.Identity_analysis(taz900, old_se_taz_832, os.path.join(scratch, '_03_taz832_taz900_identity'))\n",
    "arcpy.management.DeleteField(taz900_identity,['SA_TAZID', 'DISTMED', 'CityArea', 'CO_NAME'], \"KEEP_FIELDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apportion the attributes from old geometry to the new ones\n",
    "\n",
    "1. The Apportion tool in arcpy is still lame \n",
    "2. Open the arcgis pro, ensure the identity layer are present in the map\n",
    "3. Delete previous apportioning files if present\n",
    "4. Paste the text from the file, \"apportion_command_for_arcgis_pro.txt\", in the arcgis pro python window to get the apportion output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.startfile(r\".\\REMM-Process-Progression-Metrics-For-Web.aprx\")\n",
    "os.startfile(r\".\\apportion_command_for_arcgis_pro_v2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apportion = arcpy.ApportionPolygon_analysis(old_se_taz_832, app_columns, identity, os.path.join(outputs, 'old_se_apportion_to_taz900.shp'), \"AREA\", \"\", \"\", \"MAINTAIN_GEOMETRIES\")\n",
    "old_taz_apportion = os.path.join(scratch,'_05_old_se_apportion_to_taz900')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dissolve to summarize at geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_columns = list(new_se.columns)[4:]\n",
    "stat_fields = [[col,'SUM'] for col in app_columns]\n",
    "\n",
    "#########\n",
    "# New\n",
    "#########\n",
    "\n",
    "# taz (9.0.0)\n",
    "new_se_taz900_dissolve = arcpy.Dissolve_management(new_se_taz_900, os.path.join(scratch, '_06_new_se_taz900_dissolve'),\n",
    "                          'SA_TAZID', stat_fields, \"MULTI_PART\")\n",
    "\n",
    "# distmed (9.0.0)\n",
    "new_se_distmed_dissolve = arcpy.Dissolve_management(new_se_taz_900, os.path.join(scratch, '_06_new_se_distmed_dissolve'),\n",
    "                          'DISTMED', stat_fields, \"MULTI_PART\")\n",
    "\n",
    "# city area\n",
    "new_se_cityarea_dissolve = arcpy.Dissolve_management(new_se_taz_900, os.path.join(scratch, '_06_new_se_cityarea_dissolve'),\n",
    "                          'CityArea', stat_fields, \"MULTI_PART\")\n",
    "\n",
    "# county\n",
    "new_se_county_dissolve = arcpy.Dissolve_management(new_se_taz_900, os.path.join(scratch, '_06_new_se_county_dissolve'),\n",
    "                          'CO_NAME', stat_fields, \"MULTI_PART\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Old\n",
    "#########\n",
    "\n",
    "# taz (9.0.0)\n",
    "old_se_taz900_dissolve = arcpy.Dissolve_management(old_taz_apportion, os.path.join(scratch, '_06_old_se_taz900_dissolve'),\n",
    "                          'SA_TAZID', stat_fields, \"MULTI_PART\")\n",
    "\n",
    "# distmed (9.0.0)\n",
    "old_se_distmed_dissolve = arcpy.Dissolve_management(old_taz_apportion, os.path.join(scratch, '_06_old_se_distmed_dissolve'),\n",
    "                          'DISTMED', stat_fields, \"MULTI_PART\")\n",
    "\n",
    "# city area\n",
    "old_se_cityarea_dissolve = arcpy.Dissolve_management(old_taz_apportion, os.path.join(scratch, '_06_old_se_cityarea_dissolve'),\n",
    "                          'CityArea', stat_fields, \"MULTI_PART\")\n",
    "\n",
    "# county\n",
    "old_se_county_dissolve = arcpy.Dissolve_management(old_taz_apportion, os.path.join(scratch, '_06_old_se_county_dissolve'),\n",
    "                          'CO_NAME', stat_fields, \"MULTI_PART\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA values in Spatially enabled dataframes (ignores SHAPE column)\n",
    "def fill_na_sedf(df_with_shape_column, fill_value=0):\n",
    "    if 'SHAPE' in list(df_with_shape_column.columns):\n",
    "        df = df_with_shape_column.copy()\n",
    "        shape_column = df['SHAPE'].copy()\n",
    "        del df['SHAPE']\n",
    "        return df.fillna(fill_value).merge(shape_column,left_index=True, right_index=True, how='inner')\n",
    "    else:\n",
    "        raise Exception(\"Dataframe does not include 'SHAPE' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in new dataframes\n",
    "new_se_taz900_dissolve_df = pd.DataFrame.spatial.from_featureclass(new_se_taz900_dissolve[0])\n",
    "new_se_distmed_dissolve_df = pd.DataFrame.spatial.from_featureclass(new_se_distmed_dissolve[0])\n",
    "new_se_cityarea_dissolve_df = pd.DataFrame.spatial.from_featureclass(new_se_cityarea_dissolve[0])\n",
    "new_se_county_dissolve_df = pd.DataFrame.spatial.from_featureclass(new_se_county_dissolve[0])\n",
    "old_se_taz900_dissolve_df = pd.DataFrame.spatial.from_featureclass(old_se_taz900_dissolve[0])\n",
    "old_se_distmed_dissolve_df = pd.DataFrame.spatial.from_featureclass(old_se_distmed_dissolve[0])\n",
    "old_se_cityarea_dissolve_df = pd.DataFrame.spatial.from_featureclass(old_se_cityarea_dissolve[0])\n",
    "old_se_county_dissolve_df = pd.DataFrame.spatial.from_featureclass(old_se_county_dissolve[0])\n",
    "\n",
    "# fill NAs\n",
    "new_se_taz900_dissolve_df = fill_na_sedf(new_se_taz900_dissolve_df)\n",
    "new_se_distmed_dissolve_df = fill_na_sedf(new_se_distmed_dissolve_df)\n",
    "new_se_cityarea_dissolve_df = fill_na_sedf(new_se_cityarea_dissolve_df)\n",
    "new_se_county_dissolve_df = fill_na_sedf(new_se_county_dissolve_df)\n",
    "old_se_taz900_dissolve_df = fill_na_sedf(old_se_taz900_dissolve_df)\n",
    "old_se_distmed_dissolve_df = fill_na_sedf(old_se_distmed_dissolve_df)\n",
    "old_se_cityarea_dissolve_df = fill_na_sedf(old_se_cityarea_dissolve_df)\n",
    "old_se_county_dissolve_df = fill_na_sedf(old_se_county_dissolve_df)\n",
    "\n",
    "# round !! come up with something better\n",
    "new_se_taz900_dissolve_df = new_se_taz900_dissolve_df.round()\n",
    "new_se_distmed_dissolve_df = new_se_distmed_dissolve_df.round()\n",
    "new_se_cityarea_dissolve_df = new_se_cityarea_dissolve_df.round()\n",
    "new_se_county_dissolve_df = new_se_county_dissolve_df.round()\n",
    "old_se_taz900_dissolve_df = old_se_taz900_dissolve_df.round()\n",
    "old_se_distmed_dissolve_df = old_se_distmed_dissolve_df.round()\n",
    "old_se_cityarea_dissolve_df = old_se_cityarea_dissolve_df.round()\n",
    "old_se_county_dissolve_df = old_se_county_dissolve_df.round()\n",
    "\n",
    "# rename SA_TAZID column\n",
    "new_se_taz900_dissolve_df.rename({'SA_TAZID':'TAZID'}, axis=1, inplace=True)\n",
    "old_se_taz900_dissolve_df.rename({'SA_TAZID':'TAZID'}, axis=1, inplace=True)\n",
    "\n",
    "del new_se_distmed_dissolve_df['SHAPE']\n",
    "del old_se_distmed_dissolve_df['SHAPE']\n",
    "del new_se_cityarea_dissolve_df['SHAPE']\n",
    "del old_se_cityarea_dissolve_df['SHAPE']\n",
    "del new_se_county_dissolve_df['SHAPE']\n",
    "del old_se_county_dissolve_df['SHAPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'SUM' from column names\n",
    "to_replace = ['SUM_'+ col for col in app_columns]\n",
    "replace_dict = dict(zip(to_replace, app_columns))\n",
    "\n",
    "new_se_taz900_dissolve_df.rename(replace_dict, axis=1, inplace=True)\n",
    "old_se_taz900_dissolve_df.rename(replace_dict, axis=1, inplace=True)\n",
    "\n",
    "new_se_distmed_dissolve_df.rename(replace_dict, axis=1, inplace=True)\n",
    "old_se_distmed_dissolve_df.rename(replace_dict, axis=1, inplace=True)\n",
    "\n",
    "new_se_cityarea_dissolve_df.rename(replace_dict, axis=1, inplace=True)\n",
    "old_se_cityarea_dissolve_df.rename(replace_dict, axis=1, inplace=True)\n",
    "\n",
    "new_se_county_dissolve_df.rename(replace_dict, axis=1, inplace=True)\n",
    "old_se_county_dissolve_df.rename(replace_dict, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for deliverables\n",
    "\n",
    "map_folder = os.path.join(outputs, \"map\")\n",
    "if not os.path.exists(map_folder):\n",
    "    os.makedirs(map_folder)\n",
    "\n",
    "chart_folder = os.path.join(outputs, \"chart\")\n",
    "if not os.path.exists(chart_folder):\n",
    "    os.makedirs(chart_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================\n",
    "# Process TAZ\n",
    "#====================\n",
    "\n",
    "# taz_folder = os.path.join(map_folder, \"TAZ\")\n",
    "# if not os.path.exists(taz_folder):\n",
    "#     os.makedirs(taz_folder)\n",
    "\n",
    "taz_gdb = os.path.join(map_folder, 'TAZ.gdb')\n",
    "if not arcpy.Exists(taz_gdb):\n",
    "    arcpy.CreateFileGDB_management(map_folder, 'TAZ.gdb')\n",
    "\n",
    "\n",
    "categories = ['HH', 'POP', 'AEMP', 'RTL', 'IND', 'OTHR', 'TPCL', 'HJI']\n",
    "for c in categories:\n",
    "    new_cols = [col for col in app_columns if col.split('_')[0] == c]\n",
    "    new_temp_df = new_se_taz900_dissolve_df[['TAZID','SHAPE'] + new_cols].copy()\n",
    "    old_temp_df = old_se_taz900_dissolve_df[['TAZID','SHAPE'] + new_cols].copy()\n",
    "    \n",
    "\n",
    "    new_new_names = [col.replace(c, 'N') for col in new_cols]\n",
    "    new_old_names = [col.replace(c, 'O') for col in new_cols]\n",
    "    new_rename_dict = dict(zip(new_cols, new_new_names))\n",
    "    old_rename_dict = dict(zip(new_cols, new_old_names))\n",
    "    new_temp_df.rename(new_rename_dict, axis=1, inplace=True)\n",
    "    old_temp_df.rename(old_rename_dict, axis=1, inplace=True)\n",
    "\n",
    "    del old_temp_df['SHAPE']\n",
    "    merged = new_temp_df.merge(old_temp_df, on=['TAZID'], how='left')\n",
    "\n",
    "    merged[new_new_names + new_old_names] = merged[new_new_names + new_old_names].fillna(value=0)\n",
    "\n",
    "    # outfile = os.path.join(taz_folder, '{}_PROJECTIONS_by_TAZ.shp'.format(c))\n",
    "    outfile = os.path.join(taz_gdb, '{}_PROJECTIONS_by_TAZ'.format(c))\n",
    "    merged.spatial.to_featureclass(location=outfile, sanitize_columns=False)\n",
    "\n",
    "    arcpy.AddField_management(outfile, \"ACRES\", \"FLOAT\")\n",
    "    exp = \"round(!SHAPE.AREA@ACRES!,2)\"\n",
    "    arcpy.CalculateField_management(outfile, \"ACRES\", exp, \"PYTHON3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================\n",
    "# Process DISTMED\n",
    "#====================\n",
    "\n",
    "distmed_gdb = os.path.join(map_folder, 'DISTMED.gdb')\n",
    "if not arcpy.Exists(distmed_gdb):\n",
    "    arcpy.CreateFileGDB_management(map_folder, 'DISTMED.gdb')\n",
    "\n",
    "categories = ['HH', 'POP', 'AEMP', 'RTL', 'IND', 'OTHR', 'TPCL', 'HJI']\n",
    "for c in categories:\n",
    "    new_cols = [col for col in app_columns if col.split('_')[0] == c]\n",
    "    new_temp_df = new_se_distmed_dissolve_df[['DISTMED'] + new_cols].copy()\n",
    "    old_temp_df = old_se_distmed_dissolve_df[['DISTMED'] + new_cols].copy()\n",
    "    \n",
    "\n",
    "    new_new_names = [col.replace(c, 'N') for col in new_cols]\n",
    "    new_old_names = [col.replace(c, 'O') for col in new_cols]\n",
    "    new_rename_dict = dict(zip(new_cols, new_new_names))\n",
    "    old_rename_dict = dict(zip(new_cols, new_old_names))\n",
    "    new_temp_df.rename(new_rename_dict, axis=1, inplace=True)\n",
    "    old_temp_df.rename(old_rename_dict, axis=1, inplace=True)\n",
    "\n",
    "    merged = new_temp_df.merge(old_temp_df, on='DISTMED', how='left')\n",
    "    merged = med_districts_sdf.merge(merged, on='DISTMED', how='left')\n",
    "\n",
    "    merged[new_new_names + new_old_names] = merged[new_new_names + new_old_names].fillna(value=0)\n",
    "    merged[new_new_names + new_old_names] = merged[new_new_names + new_old_names].round(0)\n",
    "\n",
    "    # outfile = os.path.join(distmed_gdb, '{}_PROJECTIONS_by_DISTMED.shp'.format(c))\n",
    "    outfile = os.path.join(distmed_gdb, '{}_PROJECTIONS_by_DISTMED'.format(c))\n",
    "    merged.spatial.to_featureclass(location=outfile, sanitize_columns=False)\n",
    "\n",
    "    arcpy.AddField_management(outfile, \"ACRES\", \"FLOAT\")\n",
    "    exp = \"round(!SHAPE.AREA@ACRES!,2)\"\n",
    "    arcpy.CalculateField_management(outfile, \"ACRES\", exp, \"PYTHON3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================\n",
    "# Process City Area\n",
    "#====================\n",
    "\n",
    "cityarea_gdb = os.path.join(map_folder, 'CITYAREA.gdb')\n",
    "if not arcpy.Exists(cityarea_gdb):\n",
    "    arcpy.CreateFileGDB_management(map_folder, 'CITYAREA.gdb')\n",
    "\n",
    "categories = ['HH', 'POP', 'AEMP', 'RTL', 'IND', 'OTHR', 'TPCL', 'HJI']\n",
    "for c in categories:\n",
    "    new_cols = [col for col in app_columns if col.split('_')[0] == c]\n",
    "    new_temp_df = new_se_cityarea_dissolve_df[['CityArea'] + new_cols].copy()\n",
    "    old_temp_df = old_se_cityarea_dissolve_df[['CityArea'] + new_cols].copy()\n",
    "    \n",
    "\n",
    "    new_new_names = [col.replace(c, 'N') for col in new_cols]\n",
    "    new_old_names = [col.replace(c, 'O') for col in new_cols]\n",
    "    new_rename_dict = dict(zip(new_cols, new_new_names))\n",
    "    old_rename_dict = dict(zip(new_cols, new_old_names))\n",
    "    new_temp_df.rename(new_rename_dict, axis=1, inplace=True)\n",
    "    old_temp_df.rename(old_rename_dict, axis=1, inplace=True)\n",
    "\n",
    "    merged = new_temp_df.merge(old_temp_df, on='CityArea', how='left')\n",
    "    merged = city_areas_sdf.merge(merged, on='CityArea', how='left')\n",
    "\n",
    "    merged[new_new_names + new_old_names] = merged[new_new_names + new_old_names].fillna(value=0)\n",
    "\n",
    "    # outfile = os.path.join(cityarea_folder, '{}_PROJECTIONS_by_CITYAREA.shp'.format(c))\n",
    "    outfile = os.path.join(cityarea_gdb, '{}_PROJECTIONS_by_CITYAREA'.format(c))\n",
    "    merged.spatial.to_featureclass(location=outfile, sanitize_columns=False)\n",
    "\n",
    "    arcpy.AddField_management(outfile, \"ACRES\", \"FLOAT\")\n",
    "    exp = \"round(!SHAPE.AREA@ACRES!,2)\"\n",
    "    arcpy.CalculateField_management(outfile, \"ACRES\", exp, \"PYTHON3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================\n",
    "# Process County\n",
    "#====================\n",
    "\n",
    "county_gdb = os.path.join(map_folder, 'COUNTY.gdb')\n",
    "if not arcpy.Exists(county_gdb):\n",
    "    arcpy.CreateFileGDB_management(map_folder, 'COUNTY.gdb')\n",
    "\n",
    "categories = ['HH', 'POP', 'AEMP', 'RTL', 'IND', 'OTHR', 'TPCL', 'HJI']\n",
    "for c in categories:\n",
    "    new_cols = [col for col in app_columns if col.split('_')[0] == c]\n",
    "    new_temp_df = new_se_county_dissolve_df[['CO_NAME'] + new_cols].copy()\n",
    "    old_temp_df = old_se_county_dissolve_df[['CO_NAME'] + new_cols].copy()\n",
    "    \n",
    "\n",
    "    new_new_names = [col.replace(c, 'N') for col in new_cols]\n",
    "    new_old_names = [col.replace(c, 'O') for col in new_cols]\n",
    "    new_rename_dict = dict(zip(new_cols, new_new_names))\n",
    "    old_rename_dict = dict(zip(new_cols, new_old_names))\n",
    "    new_temp_df.rename(new_rename_dict, axis=1, inplace=True)\n",
    "    old_temp_df.rename(old_rename_dict, axis=1, inplace=True)\n",
    "\n",
    "    merged = new_temp_df.merge(old_temp_df, on='CO_NAME', how='left')\n",
    "    merged = counties_sdf.merge(merged, on='CO_NAME', how='left')\n",
    "\n",
    "    merged[new_new_names + new_old_names] = merged[new_new_names + new_old_names].fillna(value=0)\n",
    "\n",
    "    outfile = os.path.join(county_gdb, '{}_PROJECTIONS_by_COUNTY'.format(c))\n",
    "    merged.spatial.to_featureclass(location=outfile, sanitize_columns=False)\n",
    "\n",
    "    arcpy.AddField_management(outfile, \"ACRES\", \"FLOAT\")\n",
    "    exp = \"round(!SHAPE.AREA@ACRES!,2)\"\n",
    "    arcpy.CalculateField_management(outfile, \"ACRES\", exp, \"PYTHON3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================\n",
    "# Process TAZ to json\n",
    "#=======================\n",
    "\n",
    "chart_taz_folder = os.path.join(chart_folder, 'TAZ')\n",
    "if not os.path.exists(chart_taz_folder):\n",
    "    os.makedirs(chart_taz_folder)\n",
    "else:\n",
    "    shutil.rmtree(chart_taz_folder)\n",
    "    os.makedirs(chart_taz_folder)\n",
    "\n",
    "new_df = new_se_taz900_dissolve_df.copy()\n",
    "old_df = old_se_taz900_dissolve_df.copy()\n",
    "id_col = 'TAZID'\n",
    "\n",
    "categories = ('HH', 'POP', 'AEMP', 'RTL', 'IND', 'OTHR', 'TPCL', 'HJI')\n",
    "cols = [col for col in new_df.columns if col.startswith(categories)]\n",
    "new_new_cols = ['NEW_' + col for col in cols]\n",
    "old_new_cols = ['OLD_' + col for col in cols]\n",
    "\n",
    "new_rename_dict = dict(zip(cols, new_new_cols))\n",
    "old_rename_dict = dict(zip(cols, old_new_cols))\n",
    "new_df.rename(new_rename_dict, axis=1, inplace=True)\n",
    "old_df.rename(old_rename_dict, axis=1, inplace=True)\n",
    "\n",
    "del new_df['SHAPE']\n",
    "del old_df['SHAPE']\n",
    "\n",
    "df = new_df.merge(old_df, on=id_col, how='left')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "        f = open(os.path.join(chart_taz_folder, \"TAZ_{}.json\".format(int(row[id_col]))), \"a\")\n",
    "        f.write(\"[\\n\")\n",
    "        \n",
    "        for cat in categories:  \n",
    "            for year in range(2019,2051):\n",
    "                \n",
    "                new_col = '_'.join(['NEW', cat, str(year)])\n",
    "                old_col = '_'.join(['OLD', cat, str(year)])\n",
    "\n",
    "                f.write(\"\\t{\\n\")\n",
    "                f.write('''\\t\\t\"C\":\"{}\",\\n'''.format(cat))\n",
    "                f.write('''\\t\\t\"Y\":{},\\n'''.format(year))\n",
    "                \n",
    "                f.write('''\\t\\t\"N\":{},\\n'''.format(row[new_col]))\n",
    "                f.write('''\\t\\t\"O\":{}\\n'''.format(row[old_col]))\n",
    "        \n",
    "                if cat == \"HJI\" and year == 2050:\n",
    "                    f.write(\"\\t}\\n\")\n",
    "                else:\n",
    "                    f.write(\"\\t},\\n\")\n",
    "\n",
    "        f.write(\"]\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================\n",
    "# Process DISTMED to json\n",
    "#==========================\n",
    "\n",
    "chart_DISTMED_folder = os.path.join(chart_folder, 'DISTMED')\n",
    "if not os.path.exists(chart_DISTMED_folder):\n",
    "    os.makedirs(chart_DISTMED_folder)\n",
    "else:\n",
    "    shutil.rmtree(chart_DISTMED_folder)\n",
    "    os.makedirs(chart_DISTMED_folder)\n",
    "\n",
    "new_df = new_se_distmed_dissolve_df.copy()\n",
    "old_df = old_se_distmed_dissolve_df.copy()\n",
    "id_col = 'DISTMED'\n",
    "\n",
    "categories = ('HH', 'POP', 'AEMP', 'RTL', 'IND', 'OTHR', 'TPCL', 'HJI')\n",
    "cols = [col for col in new_df.columns if col.startswith(categories)]\n",
    "new_new_cols = ['NEW_' + col for col in cols]\n",
    "old_new_cols = ['OLD_' + col for col in cols]\n",
    "\n",
    "new_rename_dict = dict(zip(cols, new_new_cols))\n",
    "old_rename_dict = dict(zip(cols, old_new_cols))\n",
    "new_df.rename(new_rename_dict, axis=1, inplace=True)\n",
    "old_df.rename(old_rename_dict, axis=1, inplace=True)\n",
    "\n",
    "df = new_df.merge(old_df, on=id_col, how='left')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "        f = open(os.path.join(chart_DISTMED_folder, \"{}_{}.json\".format(id_col,int(row[id_col]))), \"a\")\n",
    "        f.write(\"[\\n\")\n",
    "        \n",
    "        for cat in categories:  \n",
    "            for year in range(2019,2051):\n",
    "                \n",
    "                new_col = '_'.join(['NEW', cat, str(year)])\n",
    "                old_col = '_'.join(['OLD', cat, str(year)])\n",
    "\n",
    "                f.write(\"\\t{\\n\")\n",
    "                f.write('''\\t\\t\"C\":\"{}\",\\n'''.format(cat))\n",
    "                f.write('''\\t\\t\"Y\":{},\\n'''.format(year))\n",
    "                \n",
    "                f.write('''\\t\\t\"N\":{},\\n'''.format(row[new_col]))\n",
    "                f.write('''\\t\\t\"O\":{}\\n'''.format(row[old_col]))\n",
    "        \n",
    "                if cat == \"HJI\" and year == 2050:\n",
    "                    f.write(\"\\t}\\n\")\n",
    "                else:\n",
    "                    f.write(\"\\t},\\n\")\n",
    "\n",
    "        f.write(\"]\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================\n",
    "# Process CITYAREA to json\n",
    "#===========================\n",
    "\n",
    "chart_CITYAREA_folder = os.path.join(chart_folder, 'CITYAREA')\n",
    "if not os.path.exists(chart_CITYAREA_folder):\n",
    "    os.makedirs(chart_CITYAREA_folder)\n",
    "else:\n",
    "    shutil.rmtree(chart_CITYAREA_folder)\n",
    "    os.makedirs(chart_CITYAREA_folder)\n",
    "\n",
    "\n",
    "new_df = new_se_cityarea_dissolve_df.copy()\n",
    "old_df = old_se_cityarea_dissolve_df.copy()\n",
    "id_col = 'CityArea'\n",
    "\n",
    "categories = ('HH', 'POP', 'AEMP', 'RTL', 'IND', 'OTHR', 'TPCL', 'HJI')\n",
    "cols = [col for col in new_df.columns if col.startswith(categories)]\n",
    "new_new_cols = ['NEW_' + col for col in cols]\n",
    "old_new_cols = ['OLD_' + col for col in cols]\n",
    "\n",
    "new_rename_dict = dict(zip(cols, new_new_cols))\n",
    "old_rename_dict = dict(zip(cols, old_new_cols))\n",
    "new_df.rename(new_rename_dict, axis=1, inplace=True)\n",
    "old_df.rename(old_rename_dict, axis=1, inplace=True)\n",
    "\n",
    "df = new_df.merge(old_df, on=id_col, how='left')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "        f = open(os.path.join(chart_CITYAREA_folder, \"{}_{}.json\".format(id_col,row[id_col])), \"a\")\n",
    "        f.write(\"[\\n\")\n",
    "        \n",
    "        for cat in categories:  \n",
    "            for year in range(2019,2051):\n",
    "                \n",
    "                new_col = '_'.join(['NEW', cat, str(year)])\n",
    "                old_col = '_'.join(['OLD', cat, str(year)])\n",
    "\n",
    "                f.write(\"\\t{\\n\")\n",
    "                f.write('''\\t\\t\"C\":\"{}\",\\n'''.format(cat))\n",
    "                f.write('''\\t\\t\"Y\":{},\\n'''.format(year))\n",
    "                \n",
    "                f.write('''\\t\\t\"N\":{},\\n'''.format(row[new_col]))\n",
    "                f.write('''\\t\\t\"O\":{}\\n'''.format(row[old_col]))\n",
    "        \n",
    "                if cat == \"HJI\" and year == 2050:\n",
    "                    f.write(\"\\t}\\n\")\n",
    "                else:\n",
    "                    f.write(\"\\t},\\n\")\n",
    "\n",
    "        f.write(\"]\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================\n",
    "# Process COUNTY to json\n",
    "#===========================\n",
    "\n",
    "chart_COUNTY_folder = os.path.join(chart_folder, 'COUNTY')\n",
    "if not os.path.exists(chart_COUNTY_folder):\n",
    "    os.makedirs(chart_COUNTY_folder)\n",
    "else:\n",
    "    shutil.rmtree(chart_COUNTY_folder)\n",
    "    os.makedirs(chart_COUNTY_folder)\n",
    "\n",
    "\n",
    "new_df = new_se_county_dissolve_df.copy()\n",
    "old_df = old_se_county_dissolve_df.copy()\n",
    "id_col = 'CO_NAME'\n",
    "\n",
    "categories = ('HH', 'POP', 'AEMP', 'RTL', 'IND', 'OTHR', 'TPCL', 'HJI')\n",
    "cols = [col for col in new_df.columns if col.startswith(categories)]\n",
    "new_new_cols = ['NEW_' + col for col in cols]\n",
    "old_new_cols = ['OLD_' + col for col in cols]\n",
    "\n",
    "new_rename_dict = dict(zip(cols, new_new_cols))\n",
    "old_rename_dict = dict(zip(cols, old_new_cols))\n",
    "new_df.rename(new_rename_dict, axis=1, inplace=True)\n",
    "old_df.rename(old_rename_dict, axis=1, inplace=True)\n",
    "\n",
    "df = new_df.merge(old_df, on=id_col, how='left')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "        f = open(os.path.join(chart_COUNTY_folder, \"{}_{}.json\".format(id_col,row[id_col])), \"a\")\n",
    "        f.write(\"[\\n\")\n",
    "        \n",
    "        for cat in categories:  \n",
    "            for year in range(2019,2051):\n",
    "                \n",
    "                new_col = '_'.join(['NEW', cat, str(year)])\n",
    "                old_col = '_'.join(['OLD', cat, str(year)])\n",
    "\n",
    "                f.write(\"\\t{\\n\")\n",
    "                f.write('''\\t\\t\"C\":\"{}\",\\n'''.format(cat))\n",
    "                f.write('''\\t\\t\"Y\":{},\\n'''.format(year))\n",
    "                \n",
    "                f.write('''\\t\\t\"N\":{},\\n'''.format(row[new_col]))\n",
    "                f.write('''\\t\\t\"O\":{}\\n'''.format(row[old_col]))\n",
    "        \n",
    "                if cat == \"HJI\" and year == 2050:\n",
    "                    f.write(\"\\t}\\n\")\n",
    "                else:\n",
    "                    f.write(\"\\t},\\n\")\n",
    "\n",
    "        f.write(\"]\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================\n",
    "# add files to zipped folder\n",
    "#==============================\n",
    "\n",
    "d = date.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file), \n",
    "                       os.path.relpath(os.path.join(root, file), \n",
    "                                       os.path.join(path, '..')))\n",
    "\n",
    "with zipfile.ZipFile(os.path.join('.\\\\Outputs','Files_For_Web_App_{}.zip'.format(d)),'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipdir(chart_folder, zipf)\n",
    "    zipdir(map_folder, zipf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_folder = os.path.join(chart_folder, c)\n",
    "# if not os.path.exists(category_folder):\n",
    "#     os.makedirs(category_folder)\n",
    "\n",
    "# m = merged.copy()\n",
    "# del m['SHAPE']\n",
    "# m.to_json('file.json', orient='records', lines=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import arcgis.features\n",
    "# from arcgis.features import GeoSeriesAccessor\n",
    "# gsa = arcgis.features.GeoSeriesAccessor(new_temp_df['SHAPE'])\n",
    "# new_temp_df['the_area'] = gsa.area\n",
    "# new_temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = m.iloc[0]\n",
    "# cols = [col for col in  m.columns if col.split('_')[0]=='N']\n",
    "# f = open(\"CITYAREA_{}.json\".format(a['CityArea']), \"a\")\n",
    "\n",
    "\n",
    "# f.write(\"[\\n\")\n",
    "\n",
    "\n",
    "# for col in cols:\n",
    "#     f.write(\"\\t{\\n\")\n",
    "#     f.write('''\\t\\t\"C\":\"{}\",\\n'''.format(c))\n",
    "#     f.write('''\\t\\t\"Y\":{},\\n'''.format(col.split('_')[1]))\n",
    "#     f.write('''\\t\\t\"V\":{}\\n'''.format(a[col]))\n",
    "    \n",
    "#     if col.split('_')[1] != \"2050\":\n",
    "#         f.write(\"\\t},\\n\")\n",
    "#     else:\n",
    "#         f.write(\"\\t}\\n\")\n",
    "\n",
    "# f.write(\"]\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3245673af07dcc28bdd829afb187282e9288a1f8195a5928b70ecba6e5973721"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
